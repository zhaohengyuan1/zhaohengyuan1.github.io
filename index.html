<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hengyuan Zhao</title>
</head>

<link rel="Shortcut Icon" href="./bird_logo.png" type="image/x-icon">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">


<style>
    * {
        /* font-family: Arial, Helvetica, sans-serif; */
        /* font-family: Helvetica, sans-serif; */
        font-family: 'Segoe UI', sans-serif;
    }
    body {
        margin: 0;
        padding: 0;
        background-color: #010409;
        color: #c9d1d9;
    }
    
    h2 {
        /* margin: 0; */
        padding: 0;
    }
    
    .myname {
        padding: 0 0 0 0;
        margin: 0.5em 0 0.5em 0;
    }
    
    a {
        text-decoration: none;
        color: #1C86EE;
    }
    
    a:hover {
        /* color: orange; */
        text-decoration: underline;
    }
    
    .main {
        padding: 1em;
        width: 65%;
        height: 100%;
        margin: 0 auto;
        /* background-color: ghostwhite; */
        /* border-color: #c9d1c9; */
        font-size: 18px;
        /* font-family: "Open Sans", "Helvetica Neue", Helvetica; */
    }
    
    .icons {
        font-size: 22px;
    }
    
    .biocontent {
        /* padding: 0 0 0 0.6em; */
        text-align: justify;
        margin: 0.5em 0 1em 0;
        font-size: 20px;
    }
    
    .self_link {
        /* padding: 0 0 0 0.6em; */
        margin: 0 0 1em 0;
        /* border: 1px solid rgba(240, 246, 252, 0.4); */
        /* border-radius: 6px; */
    }
    
    .h2_title {
        padding: 0 0 0.2em 0;
        /* border: 1px solid #30363d;
        border-radius: 6px; */
        border-bottom: 2px solid #30363d;
    }
    
    .news_list {
        padding: 0 0 0 1.3em;
    }
    
    table {
        border-spacing: 0 1em; 
    }

    .paperCat {
        padding: 0;
        margin: 0;
        font-size: 22px;
        font-weight: 700;
        color:burlywood;
    }

    .imagetd {
        /* width: ; */
        padding: 0 1em 0 0.5em;
        border-left: 2px solid #fafafa;
        text-align: center;
    }

    .pub_img {
        width: 300px;
        border-radius: 5px;
    }
    
    .paper_info {
        text-align: left;
        vertical-align: middle;
    }

    .paper_title {
        font-size: 20px;
        /* color: #1C86EE; */
        color: #fafafa;
        margin: 0 0 0.5em 0;
        font-weight: bold;
    }

    .paper_author {
        font-size: 18px;
        margin: 0 0 0.5em 0;
    }
    
    .paper_pub {
        font-size: 18px;
        margin: 0 0 0.5em 0;
        color: orange;
        font-weight: 800;
    }

    .paper_decription {
        font-size: 18px;
        margin: 0 0 0.5em 0;
        /* color: rgb(170,170,170); */
        color:burlywood;
    }
    
    .paper_links {
        margin: 0 0 0.5em 0;
    }
</style>


<body>
    <div class="page">
        <div class="main">
            <h2 class="myname">Henry Hengyuan Zhao (ËµµÊÅíËøú)</h2>
            
            <img src="me.jpg" width="200px">
            <div class="biocontent">
                Hiüëã, this is Henry. I'm a PhD student at <a href="https://sites.google.com/view/showlab">Show Lab</a>, National University of Singapore, under the supervision of Prof. <a href="https://sites.google.com/view/showlab/home?authuser=0">Mike Zheng Shou</a>. 
            </div>

            <div class="biocontent">
                My research interests lie broadly in Large Multimodal Models (LMM) and Human-AI Interaction (HAI). 
                Recently, I have been developing AI agent capable of interacting with computers to address real-world challenges 
                (<a href="https://arxiv.org/abs/2502.08047">WorldGUI</a>). 
                I am also interested in training LMMs to uncover its potential roles for making it intelligently 
                (<a href="https://arxiv.org/abs/2312.06731">Genixer</a>, <a href="https://arxiv.org/abs/2405.14974">LOVA3</a>) 
                or exploring their intelligence for future alignment (<a href="https://arxiv.org/abs/2502.15027">InterFeedback</a>).
            </div>

            <div class="self_link">
                <span>Links: </span>
                <span>
                    <a href="mailto:zhaohengyuan99@gmail.com"><i class="fa fa-envelope icons"></i> Email</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://github.com/zhaohengyuan1"><i class="fa-brands fa-github"></i> GitHub</a>
                </span>
                <span>/</span>
                <span>       
                    <a href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN"><i class="ai ai-google-scholar icons"></i> Google Scholar</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://twitter.com/ZHHHYuan"><i class="fa-brands fa-x-twitter"></i> @ZHHHYuan</a>
                </span>
                <span>/</span>
                <span>
                    <a href="./files/Henry_Hengyuan_Zhao_CV.pdf"><i class="fa-solid fa-file"></i> CV</a>
                </span>
            </div>

            <!-- <div class="biocontent">
                <p style="color:antiquewhite">I am seeking research collaboration opportunities, please email me if you are interested.</p>
            </div> -->

            <!-- <h2 class="h2_title">Experience</h2>
            <div>
                <strong>06/2021-present:</strong> I join SenseTime Inc. as a research intern and work with Fan Zhang.
            </div>
            <div>
                <strong>12/2020-06/2021:</strong> I join the Vision Technology (VIS), Baidu Inc. as a research intern and work with
                <a href="https://whwu95.github.io/">Wenhao Wu</a>.
            </div>
            <div>
                <strong>09/2019-Now:</strong> I am a research assistant at SIAT and supervised by Prof.
                <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN">Chao Dong</a> and <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao</a>.
            </div>
            <div>
                <strong>09/2016-06/2020:</strong> I was a undergraduate student at Nanjing University of Posts and Telecommunications, Nanjing, China.
            </div> -->

            <h2 class="h2_title">üì¢ News</h2>

            <ul class="news_list">
                <li>
                    <span>[03/2025] <a href="https://arxiv.org/abs/2502.15027v1">InterFeedBack</a> is accepted by ICLR 2025 Bidirectional Human-AI Alignment Workshop.</span>
                </li>
                <li>
                    <span>[02/2025] We released <a href="https://arxiv.org/abs/2502.15027v1">InterFeedBack</a> to explore the question "Can Large Multimodal Models evolve through Interactive Human Feedback?" Check it out!</span>
                </li>
                <li>
                    <span>[02/2025] Check our new preprint about <a href="https://arxiv.org/abs/2502.08047">WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation</a></span>
                </li>
                <li>
                    <span>[12/2024] I will present our NeurIPS paper <a href="https://arxiv.org/abs/2405.14974">LOVA3</a> at Vancouver.</span>
                </li>
                <li>
                    <span>[10/2024] I will present our ECCV paper <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03355.pdf">Genixer</a> at Milano.</span>
                </li>
                <li>
                    <span>
                        [09/2024] One paper is accpeted by NeurIPS 2024.
                    </span>
                </li>
                <li>
                    <span>
                        [07/2024] One paper is accpeted by ECCV 2024.
                    </span>
                </li>
                <li>
                    <span>
                        [09/2023] One paper is accpeted by IJCV 2023.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2023] One paper is accpeted by TPAMI 2023.
                    </span>
                </li>
            </ul>

            <h2 class="h2_title">üå∫ Research Papers</h2>

            <h3>See full publications <a href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN">here</a>.</h3>

            <table>
                <tr>
                    <td class="paperCat" colspan="2">Human-AI Interaction: </td>
                </tr>
                
                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/InterFeedback.jpg" alt="" class="pub_img" style="width: 200px;"></td>
                    <td class="paper_info">
                        <div class="paper_title">InterFeedback: Unveiling Interactive Intelligence of Large Multimodal
                            Models via Human Feedback</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao*</b>, Wenqi Pei*, Yifei Tao*, Haiyang Mei, Mike Zheng Shou *Equal contribution</div> 
                        <div class="paper_pub">ICLR 2025@Bi-Align Workshop</div>
                        <div class="paper_decription" style="color:aqua;">Can Large Multimodal Models evolve through Interactive Human Feedback? <br>We build a straightforward interactive framework that can 
                            bootstrap any LMM into an interactive process to solve problems. On top of this, we present InterFeedback-Bench, a benchmark for evaluating interactive
                            intelligence of current LMMs.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2502.15027v1">[Paper]</a>
                            </span>
                            <!-- <span>|</span> -->
                            <!-- <span>
                                <a class="paper_link" href="">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">Project Pages</a>
                            </span> -->
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">Human-Agent-Computer Interaction: </td>
                </tr>

                <tr class="pub_tr">
                    
                    <td class="imagetd"><img src="./publication_imgs/WorldGUI.jpg" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Difei Gao, Mike Zheng Shou</div>
                        <div class="paper_pub">arxiv, 2025</div>
                        <div class="paper_decription" style="color:aqua;">Benchmark: An early work for testing GUI agents in a dynamic setting. <br>Agent: An effective and foundament agent framwork for GUI automation building uppn critic-thinking philosophy.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2502.08047">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/showlab/WorldGUI">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://showlab.github.io/WorldGUI">Project Pages</a>
                            </span>
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">The roles of MLLMs: </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/lova3_neurips2024.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">LOVA3: Learning to Visual Question Answering, Asking and Assessment</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou</div>
                        <div class="paper_pub">NeurIPS 2024</div>
                        <div class="paper_decription" style="color:aqua;">üå∫ Only VQA? Let's think about GenQA and EvalQA when training MLLMs? We human not only can answering question, but also ask questions and assess the answers provided by ourself.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2405.14974">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/showlab/LOVA3">Codes</a>
                                <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/showlab/LOVA3"> -->
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/eccv2024_poster.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pan Zhou, Mike Zheng Shou</div>
                        <div class="paper_pub">ECCV 2024</div>
                        <div class="paper_decription" style="color:aqua;">üí° How MLLMs perform in data generation? (We are the first work.) We developed two data generators for nine vision-language (VL) tasks.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2312.06731">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/Genixer">Codes</a>
                                <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaohengyuan1/Genixer"> -->
                            </span>
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">Parameter-Efficient Tuning: </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/sct_ijcv2023.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou</div>
                        <div class="paper_pub">IJCV 2023</div>
                        <div class="paper_decription" style="color:aqua;">We found that tuning only a small number of task-specific channels, referred to as salient channels, is sufficient. This work represents a remarkable reduction of 780x in parameter costs compared to its full fine-tuning counterpart.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2309.08513">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/SCT">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="paperCat">Low-level Vision: </td>
                </tr>
                
                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/srga_tpami2023.jpg" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">Evaluating the Generalization Ability of Super-Resolution Networks</div>
                        <div class="paper_author">Yihao Liu, <b style="color:orange">Hengyuan Zhao</b>, Jinjin Gu, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub">TPAMI 2023</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2205.07019">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/lyh-18/SRGA">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/Color2Style.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao*</b>, Wenhao Wu*, Yihao Liu*, Dongliang He. * indicates equal contribution.</div>
                        <div class="paper_pub"> <i>Arxiv.</i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2106.08017.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/Color2Style">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr> -->

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/ClassSR_CVPR2021.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <!-- <a href="" class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a> -->
                        <div class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</div>
                        <div class="paper_author">Xiangtao Kong, <b style="color:orange">Hengyuan Zhao</b>, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub">CVPR 2021</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2103.04039.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/Xiangtaokong/ClassSR">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/pan_ECCV2020.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <!-- <a href="" class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</a> -->
                        <div class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Xiangtao Kong, Jingwen He, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub">ECCVW 2020</div>
                        <div class="paper_decription"> Over 400 citations</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2010.01073.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/PAN">Codes</a>
                                <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaohengyuan1/PAN"> -->
                            </span>
                        </div>
                    </td>
                </tr>

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/SDNet_ICCV2019.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">A Simple and Robust Deep Convolutional Approach to Blind Image Denoising</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Wenze Shao, Bingkun Bao, Haibo Li</div>
                        <div class="paper_pub"> <i>International Conference on Computer Vision Workshops <span style="color: orange;">(ICCVW 2019)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Zhao_A_Simple_and_Robust_Deep_Convolutional_Approach_to_Blind_Image_ICCVW_2019_paper.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/SDNet">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr> -->

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/CSRNet_pami.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">Very Lightweight Photo Retouching Network with Conditional Sequential Modulation</div>
                        <div class="paper_author">Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, <b style="color:orange">Hengyuan Zhao</b>, Chao Dong, Yu Qiao</div>                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2104.06279.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/hejingwenhejingwen/CSRNet">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr> -->
            </table>

        </div>
    </div>
</body>

</html>