<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Henry Hengyuan Zhao</title>
  <meta name="description" content="Henry Hengyuan Zhao ‚Äî PhD at NUS Show Lab. Multimodal AI & Agents: GUI automation, chart-to-code, interactive alignment. Open to industry Research Scientist roles in May 2026." />
  <link rel="icon" href="./bird_logo.png" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
  <style>
    * { font-family: 'Segoe UI', system-ui, -apple-system, Helvetica, Arial, sans-serif; }
    :root{
      --bg:#010409; --fg:#c9d1d9; --muted:#8b949e; --link:#1C86EE; --card:#0d1117;
      --border:#30363d; --accent:#b64ff3; --success:#73D9C6; --title:#D3D4D6; --paper-title:#84A4E8;
    }
    body { margin:0; background:var(--bg); color:var(--fg); }
    a { color:var(--link); text-decoration:none; }
    a:hover { text-decoration:underline; }
    .page { display:block; }
    .main {
      padding: 1.25rem;
      width: min(1100px, 92vw);
      margin: 0 auto;
      font-size: 18px;
    }
    h1, h2 { margin: .6rem 0 .4rem; }
    h1#name { font-size: 1.5rem; margin-top:.2rem; }

    /* ËÆ©Â∑¶ÂàóÈöèÂõæÁâáËá™Âä®ÂèòÂÆΩÔºåÈÅøÂÖçÂõæÁâáÊ∫¢Âá∫Ë¶ÜÁõñÊñáÂ≠ó */
    .top{
    display: grid;
    grid-template-columns: auto 1fr;   /* ÂÖ≥ÈîÆÔºöÂ∑¶Âàó=ÂõæÁâáÂÆûÈôÖÂÆΩÂ∫¶ */
    gap: 1.2rem;
    align-items: start;
    }

    /* Â§¥ÂÉèÔºöÊõ¥Â§ß‰ΩÜ‰∏çÊ∫¢Âá∫ */
    .portrait img{
    display: block;
    width: clamp(220px, 26vw, 300px);  /* ÊúÄÂ∞è220ÔºåÂ∏∏Áî®‚âà26vwÔºåÊúÄÂ§ß300 */
    height: auto;
    border-radius: 12px;
    border: 1px solid var(--border);
    max-width: 100%;
    }

    /* ÁßªÂä®Á´ØÊî∂Á™Ñ‰∏Ä‰∫õ */
    @media (max-width: 760px){
    .top{ grid-template-columns: 1fr; }     /* Â†ÜÂè† */
    .portrait img{ width: 200px; }
    }

    /* Âè≥‰æß‰ªãÁªçÂå∫Êï¥‰ΩìÁº©Â∞èÂ≠óÂè∑ÔºàÂè™ÂΩ±ÂìçÈ°∂ÈÉ®‰ªãÁªçÔºå‰∏çÂä®ËÆ∫ÊñáÂàóË°®Ôºâ */
    .intro .biocontent{ 
    font-size: 16px;          /* ÂéüÊù•ÊòØ 20px -> Â∞è‰∏ÄÂè∑ */
    line-height: 1.55;
    }

    /* ‰∏ÄÁ∫ßÊ†áÈ¢ò‰πüÁ®çÂæÆÂ∞è‰∏ÄÁÇπÔºåÊõ¥ÂçèË∞É */
    #name{ font-size: 1.9rem; }

    .biocontent{ margin:.5rem 0 1rem; font-size:20px; line-height:1.6; text-align:justify; }
    .badge {
      display:inline-block; padding:.18rem .6rem; border-radius:999px; border:1px solid #4b1f66;
      color:var(--accent); background:#1a1a1a; font-size:.9rem; margin-right:.5rem;
    }
        /* ÊõøÊç¢/ËøΩÂä† */
    .cta{ gap: .35rem; }                 /* ÊåâÈíÆ‰πãÈó¥Êõ¥Á¥ß */
    .btn{
    padding: .30rem .56rem;            /* Áî± ~0.42rem Áº©Â∞è */
    font-size: 1rem;                 /* Â≠ó‰ΩìÁï•Â∞è */
    border-radius: 8px;                /* Á®çÂæÆÊõ¥Á¥ßÂáëÁöÑÂúÜËßí */
    border: 1px solid #3a3a3a;
    background:#111318;
    color:#ddd;
    }
    .btn.primary{                         /* ‰∏ªÊåâÈíÆ‰πü‰∏ÄËµ∑ÂèòÁ¥ßÂáë */
    background: var(--accent);
    color: #0b0b0b;
    border-color: var(--accent);
    }
    .icons{ font-size: 16px; }            /* ÂõæÊ†áÂ∞è‰∏ÄÂè∑ */
    .badge{                               
        font-size: .85rem;                  /* Hiring ÂæΩÁ´†‰πüÂ∞è‰∏ÄÁÇπ */
        padding: .14rem .5rem;
    }

    .h2_title { padding-bottom:.25rem; border-bottom:2px solid var(--border); }
    .news_list { padding-left:1.2em; }
    .paperCat { margin:0; font-size:22px; font-weight:700; color:var(--title); }
    table { border-spacing:0 1em; width:100%; }
    .imagetd { padding:0 1em 0 .5em; border-left:2px solid #fafafa0f; text-align:center; }
    .pub_img { width:260px; }
    .paper_info { text-align:left; vertical-align:middle; }
    .paper_title { font-size:20px; color:var(--paper-title); margin:0 0 .5em; font-weight:700; }
    .paper_author { font-size:18px; margin:0 0 .5em; }
    .namecolor { color:#F9957C; }
    .paper_pub { font-size:18px; margin:0 0 .5em; color:var(--success); font-weight:800; }
    .paper_decription { font-size:18px; margin:0 0 .5em; color:#C8C0D1; }
    .paper_links { margin:0 0 .5em; }
    .self_link { margin:.6rem 0 1.2rem; }
  </style>
</head>
<body>
  <div class="page">
    <main class="main" role="main">
      <div class="top">
        <div class="portrait">
          <img src="me.jpg" alt="Portrait of Henry Hengyuan Zhao" />
        </div>
        <div>
          <h1 id="name">Henry Hengyuan Zhao</h1>

          <div class="biocontent">
            Hi üëã, I'm Henry, a PhD student in the
            <a href="https://sites.google.com/view/showlab" target="_blank" rel="noopener">Show Lab</a>
            at the National University of Singapore, advised by
            <a href="https://sites.google.com/view/showlab/home?authuser=0" target="_blank" rel="noopener">Prof. Mike Zheng Shou</a>.
          </div>

          <div class="biocontent">
            I train Large Multimodal Models (LMMs) and develop agents/benchmarks for
            GUI automation and chart-to-code.
          </div>

          <div class="biocontent">
            <strong>Selected work:</strong> agent‚Äìcomputer interaction
            (<a href="https://arxiv.org/abs/2502.08047" target="_blank" rel="noopener">WorldGUI</a>),
            large multimodal model training
            (<a href="https://arxiv.org/abs/2312.06731" target="_blank" rel="noopener">Genixer</a>,
             <a href="https://arxiv.org/abs/2405.14974" target="_blank" rel="noopener">LOVA3</a>),
            and interactive alignment/evaluation
            (<a href="https://arxiv.org/abs/2502.15027" target="_blank" rel="noopener">InterFeedback</a>).
          </div>

          <div class="biocontent">
            <span style="color: #b64ff3">I am currently on the job market, seeking research scientist positions.</span> <a class="btn primary" href="files/Henry_Hengyuan_Zhao_CV.pdf"><i class="fa-solid fa-file-lines icons"></i> CV</a>
          </div>

          <div class="biocontent">
            <!-- <span class="badge" aria-label="Hiring availability">Open to Industry ¬∑ Research Scientist ¬∑ May 2026</span> -->
            <span class="cta" aria-label="Quick actions">
              
              <a class="btn" href="mailto:zhaohengyuan99@gmail.com"><i class="fa fa-envelope icons"></i> Email</a>
              <a class="btn" href="https://github.com/zhaohengyuan1" target="_blank" rel="noopener"><i class="fa-brands fa-github"></i> GitHub</a>
              <a class="btn" href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN" target="_blank" rel="noopener"><i class="ai ai-google-scholar icons"></i> Google Scholar</a>
              <a class="btn" href="https://twitter.com/ZHHHYuan" target="_blank" rel="noopener"><i class="fa-brands fa-x-twitter"></i> @ZHHHYuan</a>
            </span>
          </div>
        </div>
      </div>

      <h2 class="h2_title">üì¢ News</h2>
      <ul class="news_list">
        <li>[08/2025] <a href="https://arxiv.org/abs/2502.15027v1">InterFeedback</a> accepted to EMNLP 2025 Findings.</li>
        <li>[03/2025] <a href="https://arxiv.org/abs/2502.15027v1">InterFeedback</a> accepted to ICLR 2025 Bi-Align Workshop (Oral).</li>
        <li>[02/2025] Released <a href="https://arxiv.org/abs/2502.15027v1">InterFeedback</a>‚Äîexploring whether LMMs evolve via interactive human feedback.</li>
        <li>[02/2025] New preprint: <a href="https://arxiv.org/abs/2502.08047">WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation</a>.</li>
        <li>[12/2024] Presenting <a href="https://arxiv.org/abs/2405.14974">LOVA3</a> at NeurIPS (Vancouver).</li>
        <li>[10/2024] Presenting <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03355.pdf">Genixer</a> at ECCV (Milan).</li>
        <li>[09/2024] One paper accepted to NeurIPS 2024.</li>
        <li>[07/2024] One paper accepted to ECCV 2024.</li>
        <li>[09/2023] One paper accepted to IJCV 2023.</li>
        <li>[08/2023] One paper accepted to TPAMI 2023.</li>
      </ul>

      <h2 class="h2_title">üå∫ Research Papers</h2>

      <!-- ===== Papers: unchanged structure, minor text fixes ===== -->
      <table role="table">
        <tr><td class="paperCat" colspan="2">Multimodal Model Alignment:</td></tr>
        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/InterFeedback.jpg" alt="InterFeedback figure" class="pub_img" style="width:200px;"></td>
          <td class="paper_info">
            <div class="paper_title">InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</div>
            <div class="paper_author"><b class="namecolor">Henry Hengyuan Zhao*</b>, Wenqi Pei*, Yifei Tao*, Haiyang Mei, Mike Zheng Shou <span class="muted">*Equal contribution</span></div>
            <div class="paper_pub">EMNLP 2025 Findings <br> ICLR 2025 @ Bi-Align Workshop (Oral)</div>
            <div class="paper_decription">We study whether LMMs can evolve via interactive human feedback and find: (1) Accuracy may not fully capture intelligence; (2) LMMs may cater to humans; (3) Low-quality feedback can hurt more than simple binary feedback.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2502.15027">Paper</a> |
              <a href="https://mp.weixin.qq.com/s/rlAEo1KsoheqJR1Z3mtqRA">[Êñ∞Êô∫ÂÖÉ]</a> |
              <a href="https://huggingface.co/datasets/hhenryz/InterFeedBack-Human">Dataset</a> |
              <a href="https://x.com/_akhaliq/status/1893854697619915119">AK's Retweet</a>
            </div>
          </td>
        </tr>

        <tr><td class="paperCat" colspan="2">Human-Agent-Computer Interaction:</td></tr>
        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/WorldGUI.jpg" alt="WorldGUI figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">WorldGUI: An Interactive Benchmark for Desktop GUI Automation from Any Starting Point</div>
            <div class="paper_author"><b class="namecolor">Henry Hengyuan Zhao</b>, Kaiming Yang, Wendi Yu, Difei Gao, Mike Zheng Shou</div>
            <div class="paper_pub">ACL 2025 @ REALM Workshop</div>
            <div class="paper_decription">The first GUI benchmark targeting dynamic planning and action-event detection for desktop automation.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2502.08047">Paper</a> |
              <a href="https://github.com/showlab/WorldGUI">Codes</a> |
              <a href="https://showlab.github.io/WorldGUI">Project Page</a>
            </div>
          </td>
        </tr>

        <tr><td class="paperCat" colspan="2">The Roles of MLLMs:</td></tr>
        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/lova3.png" alt="LOVA3 figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">LOVA3: Learning to Visual Question Answering, Asking and Assessment</div>
            <div class="paper_author"><b class="namecolor">Henry Hengyuan Zhao</b>, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou</div>
            <div class="paper_pub">NeurIPS 2024</div>
            <div class="paper_decription">Beyond answering: we introduce asking and assessing during training and obtain consistent gains without extra annotation or hyper-parameter tuning.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2405.14974">Paper</a> |
              <a href="https://github.com/showlab/LOVA3">Codes</a>
            </div>
          </td>
        </tr>

        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/genixer.png" alt="Genixer figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator</div>
            <div class="paper_author"><b class="namecolor">Henry Hengyuan Zhao</b>, Pan Zhou, Mike Zheng Shou</div>
            <div class="paper_pub">ECCV 2024</div>
            <div class="paper_decription">First to examine MLLMs for data generation‚Äîshowing diverse data synthesis and measurable downstream gains.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2312.06731">Paper</a> |
              <a href="https://github.com/zhaohengyuan1/Genixer">Codes</a>
            </div>
          </td>
        </tr>

        <tr><td class="paperCat" colspan="2">Parameter-Efficient Tuning:</td></tr>
        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/sct.jpg" alt="SCT figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</div>
            <div class="paper_author"><b class="namecolor">Henry Hengyuan Zhao</b>, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou</div>
            <div class="paper_pub">IJCV 2023</div>
            <div class="paper_decription">Tuning only a small set of salient channels achieves up to 780√ó parameter reduction vs. full fine-tuning.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2309.08513">Paper</a> |
              <a href="https://github.com/zhaohengyuan1/SCT">Codes</a>
            </div>
          </td>
        </tr>

        <tr><td class="paperCat">Low-level Vision:</td></tr>
        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/srga_tpami2023.jpg" alt="SRGA figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">Evaluating the Generalization Ability of Super-Resolution Networks</div>
            <div class="paper_author">Yihao Liu, <b class="namecolor">Hengyuan Zhao</b>, Jinjin Gu, Yu Qiao, Chao Dong</div>
            <div class="paper_pub">TPAMI 2023</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2205.07019">Paper</a> |
              <a href="https://github.com/lyh-18/SRGA">Codes</a>
            </div>
          </td>
        </tr>

        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/ClassSR_CVPR2021.png" alt="ClassSR figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</div>
            <div class="paper_author">Xiangtao Kong, <b class="namecolor">Hengyuan Zhao</b>, Yu Qiao, Chao Dong</div>
            <div class="paper_pub">CVPR 2021</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2103.04039.pdf">Paper</a> |
              <a href="https://github.com/Xiangtaokong/ClassSR">Codes</a>
            </div>
          </td>
        </tr>

        <tr class="pub_tr">
          <td class="imagetd"><img src="./publication_imgs/pan_ECCV2020.png" alt="PAN figure" class="pub_img"></td>
          <td class="paper_info">
            <div class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</div>
            <div class="paper_author"><b class="namecolor">Hengyuan Zhao</b>, Xiangtao Kong, Jingwen He, Yu Qiao, Chao Dong</div>
            <div class="paper_pub">ECCVW 2020</div>
            <div class="paper_decription">Over 400 citations.</div>
            <div class="paper_links">
              <a href="https://arxiv.org/abs/2010.01073.pdf">Paper</a> |
              <a href="https://github.com/zhaohengyuan1/PAN">Codes</a>
            </div>
          </td>
        </tr>
      </table>
    </main>
  </div>
</body>
</html>
