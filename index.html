<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hengyuan Zhao</title>
</head>

<link rel="Shortcut Icon" href="./bird_logo.png" type="image/x-icon">
<link rel="stylesheet" href="./font-awesome-4.7.0/css/font-awesome.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
<!-- <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css"> -->

<style>
    * {
        /* font-family: Arial, Helvetica, sans-serif; */
        /* font-family: Helvetica, sans-serif; */
        font-family: 'Segoe UI', sans-serif;
    }
    body {
        margin: 0;
        padding: 0;
        background-color: #010409;
        color: #c9d1d9;
    }
    
    h2 {
        /* margin: 0; */
        padding: 0;
    }
    
    .myname {
        padding: 0 0 0 0;
        margin: 0.5em 0 0.5em 0;
    }
    
    a {
        text-decoration: none;
        color: #1C86EE;
    }
    
    a:hover {
        /* color: orange; */
        text-decoration: underline;
    }
    
    .main {
        padding: 1em;
        width: 70%;
        height: 100%;
        margin: 0 auto;
        /* background-color: ghostwhite; */
        /* border-color: #c9d1c9; */
        font-size: 18px;
        /* font-family: "Open Sans", "Helvetica Neue", Helvetica; */
    }
    
    .icons {
        font-size: 22px;
    }
    
    .biocontent {
        /* padding: 0 0 0 0.6em; */
        text-align: justify;
        margin: 0.5em 0 1em 0;
        font-size: 20px;
    }
    
    .self_link {
        /* padding: 0 0 0 0.6em; */
        margin: 0 0 1em 0;
        /* border: 1px solid rgba(240, 246, 252, 0.4); */
        /* border-radius: 6px; */
    }
    
    .h2_title {
        padding: 0 0 0.2em 0;
        /* border: 1px solid #30363d;
        border-radius: 6px; */
        border-bottom: 2px solid #30363d;
    }
    
    .news_list {
        padding: 0 0 0 1.3em;
    }
    
    table {
        border-spacing: 0 1em; 
    }

    .paperCat {
        padding: 0;
        margin: 0;
        font-size: 22px;
        font-weight: 700;
        color:burlywood;
    }

    .imagetd {
        /* width: ; */
        padding: 0 1em 0 0.5em;
        border-left: 2px solid #fafafa;
        text-align: center;
    }

    .pub_img {
        width: 280px;
        border-radius: 5px;
    }
    
    .paper_info {
        text-align: left;
        vertical-align: middle;
    }

    .paper_title {
        font-size: 20px;
        /* color: #1C86EE; */
        color: #fafafa;
        margin: 0 0 0.2em 0;
        font-weight: bold;
    }

    .paper_author {
        font-size: 16px;
        margin: 0 0 0.5em 0;
    }
    
    .paper_pub {
        font-size: 16px;
        margin: 0 0 0.5em 0;
    }

    .paper_decription {
        font-size: 16px;
        margin: 0 0 0.5em 0;
        /* color: rgb(170,170,170); */
        color:burlywood;
    }
    
    .paper_links {
        margin: 0 0 0.5em 0;
    }
</style>


<body>
    <div class="page">
        <div class="main">
            <h2 class="myname">Henry Hengyuan Zhao (ËµµÊÅíËøú)</h2>
            
            <img src="me.jpg" width="200px">
            <div class="biocontent">
                Hiüëã, this is Henry. I'm a final-year PhD student at <a href="https://sites.google.com/view/showlab">Show Lab</a>, National University of Singapore, under the supervision of Prof. <a href="https://sites.google.com/view/showlab/home?authuser=0">Mike Zheng Shou</a>. In the past time, I've been fortunate to work with <a href="https://panzhous.github.io/">Pan Zhou</a>. 
            </div>

            <div class="biocontent">
                I am generally interested in multimodal understanding and Human-AI Interaction. My recent focus is on building intelligent AI for solving real-world problems and exploring the future role of current AI model.
            </div>

            <div style="margin: 0.5em 0 1em 0"><q>1. Define a right problem to work on. 2. Solve it from the first principle.</q>
            </div>
            <div class="self_link">
                <span>Links: </span>
                <span>
                    <a href="mailto:zhaohengyuan99@gmail.com"><i class="fa fa-envelope icons"></i> Email</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://github.com/zhaohengyuan1"><i class="fa fa-github icons" aria-hidden="true"></i> GitHub</a>
                </span>
                <span>/</span>
                <span>       
                    <a href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN"><i class="ai ai-google-scholar icons"></i> Google Scholar</a>
                </span>
                <span>/</span>
                <span>
                    <!-- <a href="https://twitter.com/ZHHHYuan"><i class="fa-brands fa-x-twitter"></i>Twitter</a> -->
                    <a class="twitter-follow-button"
  href="https://twitter.com/ZHHHYuan">Follow @ZHHHYuan</a>
                </span>
            </div>

            <div class="biocontent">
                <p style="color:antiquewhite">I am seeking research collaboration opportunities, please email me if you are interested.</p>
            </div>

            <!-- <h2 class="h2_title">Research Brief</h2>
            <div style="text-align:justify;">
                My primary research interests include low-level vision and generative models. The expectation of my research is that the work should be simple and the topic should be practical. The research work should be better to reveal the essence of problems. In
                my research experience, I have done two research works of efficient image super-resolution for providing the solution to saving the processing time. One work is published in AIM2020, which introduces a simple design of attention mechanism
                named "Pixel Attention". By virtue of it, we obtain fourth place in the Efficient Super-Resolution Challenge in AIM2020 among all 150 participants and obtain the lowest parameters, 272K. Moreover, considering the redundant of processing
                a large image (above 2K), we propose a framework called "ClassSR" which incoporates the classifcation network into our algorithm to help other super-resolution methods save more FLOPs with higher performance. In addition, I also spend
                the efforts of fully automatic colorization and deep exemplar-based colorization. To sovle the inconsistency of colorizing video clips by using a worthy image colorization approach, we introduce a novel and general framework which adopts
                deep feature propagation scheme to generate consecutive colorized video. Moreover, due to the unsatisfactory performance, I propose a very simple idea to implement the deep exemplar-based colorization.
            </div> -->

            <!-- <h2 class="h2_title">Experience</h2>
            <div>
                <strong>06/2021-present:</strong> I join SenseTime Inc. as a research intern and work with Fan Zhang.
            </div>
            <div>
                <strong>12/2020-06/2021:</strong> I join the Vision Technology (VIS), Baidu Inc. as a research intern and work with
                <a href="https://whwu95.github.io/">Wenhao Wu</a>.
            </div>
            <div>
                <strong>09/2019-Now:</strong> I am a research assistant at SIAT and supervised by Prof.
                <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN">Chao Dong</a> and <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao</a>.
            </div>
            <div>
                <strong>09/2016-06/2020:</strong> I was a undergraduate student at Nanjing University of Posts and Telecommunications, Nanjing, China.
            </div> -->

            <h2 class="h2_title">üì¢ News</h2>

            <ul class="news_list">
                <li>
                    <span>[12/2024] I will present our NeurIPS paper <a href="https://arxiv.org/abs/2405.14974">LOVA3</a> at Vancouver.</span>
                </li>
                <li>
                    <span>[10/2024] I will present our ECCV paper <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03355.pdf">Genixer</a> at Milano.</span>
                </li>
                <li>
                    <span>
                        [09/2024] One paper is accpeted by NeurIPS 2024.
                    </span>
                </li>
                <li>
                    <span>
                        [07/2024] One paper is accpeted by ECCV 2024.
                    </span>
                </li>
                <li>
                    <span>
                        [09/2023] One paper is accpeted by IJCV 2023.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2023] One paper is accpeted by TPAMI 2023.
                    </span>
                </li>
                <!-- <li>
                    <span>
                        [06/2021] Join SenseTime as an intern.
                    </span>
                </li>
                <li>
                    <span>
                        [03/2021] One paper is accepted by CVPR, 2021.
                    </span>
                </li>
                <li>
                    <span>
                        [12/2020] Join VIS, Baidu, worked with Wenhao WU.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2020] One paper is accepted by ECCV Workshops, 2020.
                    </span>
                </li>
                <li>
                    <span>
                        [05/2020] Participate the Efficient Super-Resoluton Challenge of AIM 2020 (ECCV Workshops). We got fourth place and lowest parameters.
                    </span>
                </li>
                <li>
                    <span>
                        [09/2019] Join MMLAB at SIAT, supervised by Yu Qiao and Chao Dong.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2019] One paper is accepted by ICCV Workshops, 2019.
                    </span>
                </li> -->
            </ul>

            <h2 class="h2_title">üå∫ Publications</h2>

            <h3>See full publications <a href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN">here</a>.</h3>

            <table>
                <tr>
                    <td class="paperCat" colspan="2">Human-AI Interaction: </td>
                </tr>
                
                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/InterFeedback.jpg" alt="" class="pub_img" style="width: 200px;"></td>
                    <td class="paper_info">
                        <div class="paper_title">InterFeedback: Unveiling Interactive Intelligence of Large Multimodal
                            Models with Human Feedback</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao*</b>, Wenqi Pei*, Yifei Tao*, Haiyang Mei, Mike Zheng Shou</div>
                        <div class="paper_pub"> *Equal contribution</div>
                        <div class="paper_pub"> <i>Arxiv preprint, Feburary 2025</i></div>
                        <div class="paper_decription" style="color:aqua;">Can Large Multimodal Models evolve through Interactive Human Feedback? <br>We build a straightforward interactive framework that can 
                            bootstracp any LMM into an interactive process to solve problems. On top of this, we present InterFeedback-Bench, a benchmark for evaluating interactive
                            intelligence of current LMMs.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="">Paper</a>
                            </span>
                            <!-- <span>|</span> -->
                            <!-- <span>
                                <a class="paper_link" href="">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">Project Pages</a>
                            </span> -->
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">Human-Agent-Computer Interaction: </td>
                </tr>

                <tr class="pub_tr">
                    
                    <td class="imagetd"><img src="./publication_imgs/WorldGUI.jpg" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Difei Gao, Mike Zheng Shou</div>
                        <div class="paper_pub"> <i>Arxiv preprint, Feburary 2025</i></div>
                        <div class="paper_decription" style="color:aqua;">Benchmark: An early work for testing GUI agents in a dynamic setting. <br>Agent: An effective and foundament agent framwork for GUI automation building uppn critic-thinking philosophy.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/abs/2502.08047">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/showlab/WorldGUI">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://showlab.github.io/WorldGUI">Project Pages</a>
                            </span>
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">The roles of MLLMs: </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/lova3_neurips2024.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">LOVA3: Learning to Visual Question Answering, Asking and Assessment</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou</div>
                        <div class="paper_pub"> <i>Neural Information Processing Systems <span style="color: orange;">(NeurIPS 2024)</span></i></div>
                        <div class="paper_decription" style="color:aqua;">üå∫ Only VQA? Let's think about GenQA and EvalQA when training MLLMs? We human not only can answering question, but also ask questions and assess the answers provided by ourself.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2405.14974">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/showlab/LOVA3">Codes</a>
                                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/showlab/LOVA3">
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/eccv2024_poster.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pan Zhou, Mike Zheng Shou</div>
                        <div class="paper_pub"> <i>European Conference on Computer Vision <span style="color: orange;">(ECCV 2024)</span></i></div>
                        <div class="paper_decription" style="color:aqua;">üí° How MLLMs perform in data generation? (We are the first work.) We developed two data generators for nine vision-language (VL) tasks.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2312.06731">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/Genixer">Codes</a>
                                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaohengyuan1/Genixer">
                            </span>
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="paperCat" colspan="2">Parameter-Efficient Tuning: </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/sct_ijcv2023.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</div>
                        <div class="paper_author"><b style="color:orange">Henry Hengyuan Zhao</b>, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou</div>
                        <div class="paper_pub"> <i>International Journal of Computer Vision <span style="color: orange;">(IJCV 2023)</span></i></div>
                        <div class="paper_decription" style="color:aqua;">We found that tuning only a small number of task-specific channels, referred to as salient channels, is sufficient. This work represents a remarkable reduction of 780x in parameter costs compared to its full fine-tuning counterpart.</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2309.08513">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/SCT">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="paperCat">Low-level Vision: </td>
                </tr>
                
                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/srga_tpami2023.jpg" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <div class="paper_title">Evaluating the Generalization Ability of Super-Resolution Networks</div>
                        <div class="paper_author">Yihao Liu, <b style="color:orange">Hengyuan Zhao</b>, Jinjin Gu, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub"> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence <span style="color: orange;">(TPAMI 2023)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2205.07019">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/lyh-18/SRGA">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/Color2Style.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao*</b>, Wenhao Wu*, Yihao Liu*, Dongliang He. * indicates equal contribution.</div>
                        <div class="paper_pub"> <i>Arxiv.</i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2106.08017.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/Color2Style">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr> -->

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/ClassSR_CVPR2021.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <!-- <a href="" class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a> -->
                        <div class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</div>
                        <div class="paper_author">Xiangtao Kong, <b style="color:orange">Hengyuan Zhao</b>, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub"> <i>Computer Vision and Pattern Recognition <span style="color: orange;">(CVPR 2021)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2103.04039.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/Xiangtaokong/ClassSR">Codes</a>
                                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/XPixelGroup/ClassSR">
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td class="imagetd"><img src="./publication_imgs/pan_ECCV2020.png" alt="" class="pub_img"></td>
                    <td class="paper_info">
                        <!-- <a href="" class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</a> -->
                        <div class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Xiangtao Kong, Jingwen He, Yu Qiao, Chao Dong</div>
                        <div class="paper_pub"> <i>European Conference on Computer Vision Workshops <span style="color: orange;">(ECCVW 2020)</span></i></div>
                        <div class="paper_decription"> Over 390 citations</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2010.01073.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/PAN">Codes</a>
                                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaohengyuan1/PAN">
                            </span>
                        </div>
                    </td>
                </tr>

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/SDNet_ICCV2019.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">A Simple and Robust Deep Convolutional Approach to Blind Image Denoising</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Wenze Shao, Bingkun Bao, Haibo Li</div>
                        <div class="paper_pub"> <i>International Conference on Computer Vision Workshops <span style="color: orange;">(ICCVW 2019)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Zhao_A_Simple_and_Robust_Deep_Convolutional_Approach_to_Blind_Image_ICCVW_2019_paper.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/SDNet">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr> -->

                <!-- <tr class="pub_tr">
                    <td><img src="./publication_imgs/CSRNet_pami.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <div class="paper_title">Very Lightweight Photo Retouching Network with Conditional Sequential Modulation</div>
                        <div class="paper_author">Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, <b style="color:orange">Hengyuan Zhao</b>, Chao Dong, Yu Qiao</div>                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2104.06279.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/hejingwenhejingwen/CSRNet">Codes</a>
                            </span>
                        </div>
                    </td>
                </tr> -->
            </table>

        </div>
    </div>
</body>

</html>