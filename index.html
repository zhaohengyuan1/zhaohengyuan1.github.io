<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hengyuan Zhao</title>
</head>

<link rel="Shortcut Icon" href="./hengyuan_icon.png" type="image/x-icon">
<link rel="stylesheet" href="./font-awesome-4.7.0/css/font-awesome.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
<!-- <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css"> -->

<style>
    body {
        margin: 0;
        padding: 0;
        background-color: #010409;
        color: #c9d1d9;
    }

    h2 {
        /* margin: 0; */
        padding: 0;
    }

    .myname {
        padding: 0 0 0 0;
        margin: 0.5em 0 0.5em 0;
    }

    a {
        text-decoration: none;
        color: #1C86EE;
    }

    a:hover {
        /* color: orange; */
        text-decoration: underline;
    }

    .main {
        padding: 1em;
        width: 1000px;
        height: 100%;
        margin: 0 auto;
        /* background-color: ghostwhite; */
        /* border-color: #c9d1c9; */
        font-size: 18px;
        /* font-family: "Open Sans", "Helvetica Neue", Helvetica; */
    }

    .icons {
        font-size: 22px;
    }

    .title {
        /* padding: 0 0 0 0.6em; */
        text-align: justify;
        margin: 0.5em 0 1em 0;
    }

    .self_link {
        /* padding: 0 0 0 0.6em; */
        margin: 0 0 1em 0;
        /* border: 1px solid rgba(240, 246, 252, 0.4); */
        /* border-radius: 6px; */
    }

    .h2_title {
        padding: 0 0 0.2em 0;
        /* border: 1px solid #30363d;
        border-radius: 6px; */
        border-bottom: 2px solid #30363d;
    }

    .news_list {
        padding: 0 0 0 1.3em;
    }

    .pub_img {
        /* width: 60%; */
        width: 260px;
        height: 150px;
        /* border: 1px solid #30363d; */
        border-radius: 6px;
    }

    /* .pub_tr {
        padding: 0 0 2em 0;
    } */

    .paper_info {
        padding: 0 0 0 0.5em;
    }

    .paper_title {
        font-size: 22px;
        color: #1C86EE;
        margin: 0 0 0.2em 0;
        /* color: orange; */
    }

    .paper_author {
        font-size: 18px;
        margin: 0 0 0.2em 0;
    }

    .paper_pub {
        font-size: 18px;
        margin: 0 0 0.2em 0;
    }

    .paper_links {
        margin: 0 0 1em 0;
    }
</style>


<body>
    <div class="page">
        <div class="main">
            <h2 class="myname">Hengyuan Zhao (赵恒远)</h2>
            <div class="title">
                Hengyuan is a research intern at <a href="https://www.sensetime.com">SenseTime</a> Inc., where he works
                with Fan Zhang. His work focuses on image denoising in low-light conditions. He previously worked as a
                research intern at VIS <a href=https://www.baidu.com/">Baidu</a> Inc., where he focused on restoring old
                films or photos using image restoration techniques like super-resolution, denoising, deblurring, and
                colorization. At the same time, he was supervised as a research assistant by <a
                    href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN">Chao Dong</a> and <a
                    href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao</a> at the <a
                    href="http://xpixel.group/people.html"> XPixel Group</a> at Shenzhen Institutes of Advanced
                Technology (SIAT).
            </div>

            <div class="self_link">
                <span>Links: </span>
                <span>
                    <a href="mailto:zhaohengyuan99@gmail.com"><i class="fa fa-envelope icons"></i> Email</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://github.com/zhaohengyuan1"><i class="fa fa-github icons" aria-hidden="true"></i>
                        GitHub</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://scholar.google.com/citations?user=QLSk-6IAAAAJ&hl=zh-CN"><i
                            class="ai ai-google-scholar icons"></i> Google Scholar</a>
                </span>
                <span>/</span>
                <span>
                    <a href="https://zhaohengyuan1.github.io/zhaohengyuan_CV_20210719.pdf"><i
                            class="ai ai-cv icons"></i></a>
                </span>
            </div>

            <!-- <h2 class="h2_title">Research Brief</h2>
            <div style="text-align:justify;">
                My primary research interests include low-level vision and generative models. The expectation of my research is that the work should be simple and the topic should be practical. The research work should be better to reveal the essence of problems. In
                my research experience, I have done two research works of efficient image super-resolution for providing the solution to saving the processing time. One work is published in AIM2020, which introduces a simple design of attention mechanism
                named "Pixel Attention". By virtue of it, we obtain fourth place in the Efficient Super-Resolution Challenge in AIM2020 among all 150 participants and obtain the lowest parameters, 272K. Moreover, considering the redundant of processing
                a large image (above 2K), we propose a framework called "ClassSR" which incoporates the classifcation network into our algorithm to help other super-resolution methods save more FLOPs with higher performance. In addition, I also spend
                the efforts of fully automatic colorization and deep exemplar-based colorization. To sovle the inconsistency of colorizing video clips by using a worthy image colorization approach, we introduce a novel and general framework which adopts
                deep feature propagation scheme to generate consecutive colorized video. Moreover, due to the unsatisfactory performance, I propose a very simple idea to implement the deep exemplar-based colorization.
            </div> -->

            <h2 class="h2_title">Experience</h2>
            <div>
                <strong>06/2021-10/2021:</strong> As a research intern, I joined SenseTime Inc.'s MIG and worked with
                Fan Zhang.
            </div>
            <div>
                <strong>12/2020-06/2021:</strong> As a research intern, I joined Baidu Inc.'s Vision Technology (VIS)
                and worked with <a href="https://whwu95.github.io/">Wenhao Wu</a>.
            </div>
            <div>
                <strong>09/2019-Present:</strong> I am a research assistant at SIAT and supervised by Prof.
                <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN">Chao Dong</a> and <a
                    href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao</a>.
            </div>
            <div>
                <strong>09/2016-06/2020:</strong> I was a undergraduate student at Nanjing University of Posts and
                Telecommunications, Nanjing, China.
            </div>

            <h2 class="h2_title">News</h2>

            <ul class="news_list">
                <li>
                    <span>
                        [06/2021] Join SenseTime, work with Fan Zhang.
                    </span>
                </li>
                <li>
                    <span>
                        [03/2021] One paper accepted by CVPR, 2021.
                    </span>
                </li>
                <li>
                    <span>
                        [12/2020] Join VIS, Baidu, worked with Wenhao WU.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2020] One paper accepted by ECCV Workshops, 2020.
                    </span>
                </li>
                <li>
                    <span>
                        [05/2020] Participate the Efficient Super-Resoluton Challenge of AIM 2020 (ECCV Workshops). We
                        got fourth place and lowest parameters.
                    </span>
                </li>
                <li>
                    <span>
                        [09/2019] Join MMLAB at SIAT, supervised by Yu Qiao and Chao Dong.
                    </span>
                </li>
                <li>
                    <span>
                        [08/2019] One paper accepted by ICCV Workshops, 2019.
                    </span>
                </li>
            </ul>

            <h2 class="h2_title">Publications</h2>

            <div class="pub_tr">
                <!-- <td><img src="./publication_imgs/color2embed.png" alt="" class="pub_img"></td> -->
                <iframe width="100%" height="600" src="https://www.youtube.com/embed/c7dczMs-olE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <td class="paper_info" valign="top" width="100%">
                    <!-- <a href="" class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a> -->
                    <div class="paper_title">Temporally Consistent Video Colorization with Deep Feature Propagation
                        and Self-regularization Learning</div>
                    <div class="paper_author">Yihao Liu* <b style="color:orange">Hengyuan Zhao*</b> Kelvin C.K.
                        Chan, Xintao Wang, Chen Change Loy, Yu Qiao, Chao Dong.</div>
                    <div class="paper_pub"> <i>Under review.</i></div>
                    <div class="paper_links">
                        <span>
                            <a class="paper_link" href="https://arxiv.org/abs/2110.04562.pdf">Paper(arxiv)</a>
                        </span>
                        <span>|</span>
                        <span>
                            <a class="paper_link"
                                href="https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization">Codes</a>
                        </span>
                        <span>|</span>
                        <span>
                            <a class="paper_link" href="">BibTex</a>
                        </span>
                    </div>
                </td>
            </div>

            <table>

                <tr class="pub_tr">
                    <td><img src="./publication_imgs/Color2Embed3.png" alt="" class="pub_img"   ></td>
                    <td class="paper_info" valign="top">
                        <!-- <a href="" class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a> -->
                        <div class="paper_title">Color2Embed: Fast Exemplar-Based Image Colorization using Color
                            Embeddings</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao*</b>, Wenhao Wu*, Yihao Liu*,
                            Dongliang He.</div>
                        <div class="paper_pub"> <i>Arxiv.</i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2106.08017.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/Color2Style">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td><img src="./publication_imgs/ClassSR_CVPR2021.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <!-- <a href="" class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a> -->
                        <div class="paper_title">ClassSR: A General Framework to Accelerate Super-Resolution Networks by
                            Data Characteristic</div>
                        <div class="paper_author">Xiangtao Kong, <b style="color:orange">Hengyuan Zhao</b>, Yu Qiao,
                            Chao Dong</div>
                        <div class="paper_pub"> <i>Computer Vision and Pattern Recognition <span
                                    style="color: orange;">(CVPR 2021)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2103.04039.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/Xiangtaokong/ClassSR">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td><img src="./publication_imgs/pan_ECCV2020.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <!-- <a href="" class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</a> -->
                        <div class="paper_title">Efficient Image Super-Resolution Using Pixel Attention</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Xiangtao Kong, Jingwen He,
                            Yu Qiao, Chao Dong</div>
                        <div class="paper_pub"> <i>European Conference on Computer Vision Workshops <span
                                    style="color: orange;">(ECCVW 2020)</span></i></div>
                        <div class="paper_pub"> We got fourth place of Efficient Image Super Resolution Challenge in
                            total 150 participants. (The lowest paramters, 272K)</div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2010.01073.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/PAN">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td><img src="./publication_imgs/SDNet_ICCV2019.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <!-- <a href="" class="paper_title">A Simple and Robust Deep Convolutional Approach to Blind Image Denoising</a> -->
                        <div class="paper_title">A Simple and Robust Deep Convolutional Approach to Blind Image
                            Denoising</div>
                        <div class="paper_author"><b style="color:orange">Hengyuan Zhao</b>, Wenze Shao, Bingkun Bao,
                            Haibo Li</div>
                        <div class="paper_pub"> <i>International Conference on Computer Vision Workshops <span
                                    style="color: orange;">(ICCVW 2019)</span></i></div>
                        <div class="paper_links">
                            <span>
                                <a class="paper_link"
                                    href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Zhao_A_Simple_and_Robust_Deep_Convolutional_Approach_to_Blind_Image_ICCVW_2019_paper.pdf">Paper</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/zhaohengyuan1/SDNet">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr>

                <tr class="pub_tr">
                    <td><img src="./publication_imgs/CSRNet_pami.png" alt="" class="pub_img"></td>
                    <td class="paper_info" valign="top">
                        <!-- <a href="" class="paper_title">Very Lightweight Photo Retouching Network with Conditional Sequential Modulation</a> -->
                        <div class="paper_title">Very Lightweight Photo Retouching Network with Conditional Sequential
                            Modulation</div>
                        <div class="paper_author">Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, <b
                                style="color:orange">Hengyuan Zhao</b>, Chao Dong, Yu Qiao</div>
                        <!-- <div class="paper_pub">European Conference on Computer Vision Workshops (ECCVW), 2020</div> -->
                        <div class="paper_links">
                            <span>
                                <a class="paper_link" href="https://arxiv.org/pdf/2104.06279.pdf">Paper(arxiv)</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="https://github.com/hejingwenhejingwen/CSRNet">Codes</a>
                            </span>
                            <span>|</span>
                            <span>
                                <a class="paper_link" href="">BibTex</a>
                            </span>
                        </div>
                    </td>
                </tr>
            </table>

            <h3 class="myname">Please contact me if you have any questions about my research.</h3>
        </div>
    </div>
</body>

</html>